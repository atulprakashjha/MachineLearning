{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 2 - Decision Trees - Student.ipynb","version":"0.3.2","provenance":[{"file_id":"1NEeJMHIfeBs0x7eIX0pXSj12qMOw99B6","timestamp":1536693193053},{"file_id":"1JCH33rfygsL_Bhi8duxnjePoVfKozltf","timestamp":1535495075614}],"collapsed_sections":["yT2ecg5qp2Mf"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"xjxEHlRMpS5G","colab_type":"text"},"cell_type":"markdown","source":["# Lab 2 - Decision Trees\n"]},{"metadata":{"id":"yT2ecg5qp2Mf","colab_type":"text"},"cell_type":"markdown","source":["##Decision Trees\n","\n","Decision trees are great because they allow us to convey complex predictive models to those uninitiated in Machine Learning. We can build a model and then \"drop\" a new observation through it (like Plinko!) until we reach an outcome -- be it quantitative or qualitative. However, remember that they have their limitations by only being able to make horizontal/vertical decision boundaries. This can limit their accuracy in real-world applications. Furthermore, they are also prone to overfitting models (more later!) unless we make some adjustments to how the algorithm runs.\n","\n","Since we are already becoming comfortable with the Boston Housing Prices data set, let's continue to use that. We converted our target variable, **MEDV**, to a binary outcome so we will do the same here and use decision trees for classification modeling.\n","\n","##Shortcuts\n","*   Use the \"+ Code\" button in the top left corner to add another block like this one only for running **code** or the \"+ text\" button for adding a block that runs **text**\n","*   I suggest looking at \"Tools-->Keyboard Shortcuts...\" for additional ways to run Colaboratory but here are a few useful ones:\n","> **Ctrl+F9** - Run all blocks   \n","> **Ctrl+Enter** - Run selected block   \n","> **Alt+Enter** - Run block and add a new block beneath   \n","> **Shift+Enter** - Run block and select next block   \n","> **Ctrl+F8** - Run all blocks before selected block   \n","> **Ctrl+F10** - Run selected block and all following blocks   \n","> **Ctrl+M+Y** - Convert selected block to a *code* block   \n","> **Ctrl+M+M** - Convert selected block to a *text* block\n","\n","Also useful, Colaboratory supports code completion. Start typing code and press the **Tab** key. A drop down will appear with likely code based on what you typed. If only one possible command exists, it should complete it for you automatically.\n","\n","Even better yet, if there is an error produced by your code, Colaboratory will provide a button at the bottom of the code output to search StackOverflow for an answer!\n","\n","**Remember**: Code blocks need to be run in succession or they might produce errors!\n"]},{"metadata":{"id":"38PcL20cxbGV","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt-get -qq install -y graphviz && pip install -q pydot\n","import pydot"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jz20WBVQyMhB","colab_type":"code","colab":{}},"cell_type":"code","source":["#Install packages not native to Colaboratory\n","!pip -q install graphviz\n","!pip -q install pydot\n","import pydot"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8RfFsPwk8o5T","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load our packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import graphviz\n","from google.colab import files\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.cluster import KMeans\n","from sklearn import datasets\n","%matplotlib inline "],"execution_count":0,"outputs":[]},{"metadata":{"id":"kf6yoJ4fH_FD","colab_type":"code","colab":{}},"cell_type":"code","source":["#Upload the 'HousingData.csv' file from your local computer \n","files.upload()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BDkNr5UiTWl8","colab_type":"code","colab":{}},"cell_type":"code","source":["#Assign data to objects\n","raw = pd.read_csv('HousingData.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7dEdVZ2FRt2q","colab_type":"code","colab":{}},"cell_type":"code","source":["#Repeat the data cleaning activites from Lab 1\n","raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']] = raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']].fillna(raw.mean())\n","\n","for col in ['ZN','CHAS']:\n","    raw[col].fillna(raw[col].mode()[0], inplace=True)\n","    \n","#Binary coding of target  \n","raw['MEDV'] = np.where(raw['MEDV'] < 21.2, 0, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YFSL6ojVO67l","colab_type":"text"},"cell_type":"markdown","source":["Let's just build a decision tree in scikit-learn using the default values for its parameters. Remember, you should always be able to check the arguments a Python function takes by putting your mouse within its parenthesis \"()\" and pressing **Tab**."]},{"metadata":{"id":"jds1oG6GHIRg","colab_type":"code","colab":{}},"cell_type":"code","source":["#Build and visualize a decision tree classifier\n","X = raw.iloc[:, :-1].values\n","y = raw['MEDV'].values\n","\n","dt = DecisionTreeClassifier()\n","dt.fit(X, y)\n","\n","dt_viz = tree.export_graphviz(dt, \n","                              out_file=None, \n","                              feature_names=raw.columns[0:13],  \n","                              class_names=['low','high'],  \n","                              filled=True, \n","                              rounded=True,\n","                              proportion=True,\n","                              special_characters=True)\n","\n","graph = graphviz.Source(dt_viz)\n","graph.format = 'png'\n","graph"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sQWFGn2-KRH0","colab_type":"text"},"cell_type":"markdown","source":["From the above, we can see a lot of the features we learned about in the decision trees lecture: \n","\n","* The order variables are being \"split\" on\n","* The Gini Impurity score (note that the darker the color of the node, the greater proportion of one class to the other)\n","* The proportion of all observations included in each node (i.e. \"samples = xx%\") \n","* The proportion of observations of each class within a node (i.e. \"value = [low, high]\")\n","* The outcome we assign to the node based on the highest occuring outcome (i.e. \"class=low\")\n","\n","We can also see that the decision tree is giant and has several branches because we built it without stopping it or pruning it which means it is very likely *overfitting* (most of the terminal nodes near the bottom only have few observations in them). Thankfully we can adjust things like:\n","\n","* `max_depth` - How *low* will we let our tree grow overall (i.e. how many levels of splits are allowed)?\n","* `min_samples_split` - What are minimal number of observations needed to be in a node for us to consider splitting further?\n","* `min_samples_leaf` - What are minimal number of observations needed to be a terminal node?\n","* `min_impurity_decrease` - What is the minimum decrease in Gini Impurity we'd need to consider splitting a node further?\n","\n","Let's play with these (and I encourage you to do the same)!"]},{"metadata":{"id":"piRUyewaQfsB","colab_type":"code","colab":{}},"cell_type":"code","source":["#Build and visualize a decision tree classifier with stopping/pruning criteria\n","dt = DecisionTreeClassifier(max_depth=4,\n","                            min_samples_leaf=25)\n","dt.fit(X, y)\n","\n","dt_viz = tree.export_graphviz(dt, \n","                              out_file=None, \n","                              feature_names=raw.columns[0:13],  \n","                              class_names=['low','high'],  \n","                              filled=True, \n","                              rounded=True,\n","                              proportion=True,\n","                              special_characters=True)\n","\n","graph = graphviz.Source(dt_viz)\n","graph.format = 'png'\n","graph"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UiOkpHkAnuPB","colab_type":"text"},"cell_type":"markdown","source":["In this example above, we set the `max_depth` to \"4\" meaning that after the first split, we won't allow more than 4 additional ones. This is visually obvious in the decision tree above. The value of \"25\" for `min_samples_leaf` indicates that we should not let our tree make a node unless at least 25 observations would fall into it. You can rebuild this tree as many times as you'd like using different parameter values or even some of the parameters we didn't use like `max_samples_leaf` and `max_impurity_decrease`.\n","\n","How well do you like the final tree you created? Why did you decide on the paramters you chose?"]},{"metadata":{"id":"_HWOgrrezy9q","colab_type":"text"},"cell_type":"markdown","source":["##Your Turn!\n","We've already loaded the **\"Admission_Predict.csv\"** above and transformed the **\"Chance to Admit\"** variable into a binary one. Let's explore it using decision trees next!\n","\n","**Build a default decision tree to predict \"Chance of Admit\" using the remaining features of the \"Admission_Predict.csv\" data set. What feature is considered the most \"important\" in the context of decision trees?**"]},{"metadata":{"id":"HMuJP_vNCI6W","colab_type":"code","colab":{}},"cell_type":"code","source":["#Upload the \"Admission_Predict.csv\" file from your local computer\n","files.upload()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tmqcb1NZDwaw","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pw6G7bfJ3Ky1","colab_type":"text"},"cell_type":"markdown","source":["**Build another tree. This time adjust the stopping/pruning parameters like we did above and find a tree you feel comfortable with. Why do you feel that way? Also, feel free to explore the other parameters within `DecisionTree()` that you can adjust to optimize your tree. Why did you choose those?**"]},{"metadata":{"id":"lm-KzXIY4QJ_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}