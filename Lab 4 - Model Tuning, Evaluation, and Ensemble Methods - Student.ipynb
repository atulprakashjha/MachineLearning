{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 4 - Model Tuning, Evaluation, and Ensemble Methods - Student.ipynb","version":"0.3.2","provenance":[{"file_id":"1If9LlGAlq552z9B4AVj80lW3vdjJd133","timestamp":1536695378063},{"file_id":"1JCH33rfygsL_Bhi8duxnjePoVfKozltf","timestamp":1536360735680}],"collapsed_sections":["yT2ecg5qp2Mf"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xjxEHlRMpS5G","colab_type":"text"},"source":["# Lab 4 - Model Tuning, Evaluation, and Ensemble Methods\n"]},{"cell_type":"markdown","metadata":{"id":"yT2ecg5qp2Mf","colab_type":"text"},"source":["##Model Tuning, Evaluation, and Ensemble Methods\n","We've learned a lot of different ML models up to this point, but we haven't had a means of testing how well they actually work. Remember, when we build models we are more concerned with how they perform on unseen data. If we are only given one data set, how can we see how our model performs on new data if we don't have any? Tuning and evaluation methods give us the means of testing our models so we can adjust their parameters for optimal performance when deployed. Let's revisit some of our earlier models but this time we'll split our data into training and testing sets so we can evaluate, iterate, and optimize our models.\n","\n","We also learned about Ensemble Methods. Ensembles are like getting a second opinion on our models -- or some times hundreds of opinions. This allows us to build several models with slightly different data, parameters, and so on. We can then average them together for a better idea of overall performance.\n","\n","##Shortcuts\n","*   Use the \"+ Code\" button in the top left corner to add another block like this one only for running **code** or the \"+ text\" button for adding a block that runs **text**\n","*   I suggest looking at \"Tools-->Keyboard Shortcuts...\" for additional ways to run Colaboratory but here are a few useful ones:\n","> **Ctrl+F9** - Run all blocks   \n","> **Ctrl+Enter** - Run selected block   \n","> **Alt+Enter** - Run block and add a new block beneath   \n","> **Shift+Enter** - Run block and select next block   \n","> **Ctrl+F8** - Run all blocks before selected block   \n","> **Ctrl+F10** - Run selected block and all following blocks   \n","> **Ctrl+M+Y** - Convert selected block to a *code* block   \n","> **Ctrl+M+M** - Convert selected block to a *text* block\n","\n","Also useful, Colaboratory supports code completion. Start typing code and press the **Tab** key. A drop down will appear with likely code based on what you typed. If only one possible command exists, it should complete it for you automatically.\n","\n","Even better yet, if there is an error produced by your code, Colaboratory will provide a button at the bottom of the code output to search StackOverflow for an answer!\n","\n","**Remember**: Code blocks need to be run in succession or they might produce errors!\n"]},{"cell_type":"code","metadata":{"id":"JUWSgvqM8Qih","colab_type":"code","colab":{}},"source":["!apt-get -qq install -y graphviz && pip install -q pydot\n","import pydot\n","\n","#Install packages not native to Colaboratory\n","!pip -q install graphviz\n","!pip -q install pydot\n","import pydot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RfFsPwk8o5T","colab_type":"code","colab":{}},"source":["#Load our packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import graphviz\n","from google.colab import files\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.cluster import KMeans\n","from sklearn import datasets\n","%matplotlib inline "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kf6yoJ4fH_FD","colab_type":"code","colab":{}},"source":["#Upload the 'HousingData.csv' file from your local computer \n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKfDEyQyI5es","colab_type":"code","colab":{}},"source":["#Assign data to objects\n","raw = pd.read_csv('HousingData.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyhBkNaNTSAR","colab_type":"code","colab":{}},"source":["#Repeat the data cleaning activites from Lab 1\n","raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']] = raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']].fillna(raw.mean())\n","\n","for col in ['ZN','CHAS']:\n","    raw[col].fillna(raw[col].mode()[0], inplace=True)   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVo14LAGToSW","colab_type":"text"},"source":["##Linear Regression - Test/Train"]},{"cell_type":"markdown","metadata":{"id":"s5TfSfqm1XFh","colab_type":"text"},"source":["Let's rerun our code for linear regression like we did in Lab 1:"]},{"cell_type":"code","metadata":{"id":"fWjEBYfrpvcs","colab_type":"code","colab":{}},"source":["#Build and visualize a simple linear regression\n","X = raw[['RM']].values \n","y = raw['MEDV'].values\n","\n","lm = LinearRegression()\n","lm.fit(X, y)\n","\n","def lin_regplot(X, y, model):\n","    plt.figure(figsize=(15,10))\n","    plt.scatter(X, y, c='navy', edgecolor='white', s=55)\n","    plt.plot(X, model.predict(X), color='black', lw=2.5)\n","    return None\n","\n","lin_regplot(X, y, lm)\n","plt.xlabel('Avg. Rooms')\n","plt.ylabel('Price in $1,000')\n","plt.title('Home Price by Avg. Rooms')\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGTIQYyNpvl8","colab_type":"text"},"source":["Remember, the first time we executed this code we used our entire data set to build the regression model. What if we were to remove some of our data set observations at random and use those to test how well our model works? Let's try that approach and see how well our model can work at predicting prices of observations that weren't used to build it. \n","\n","Since we only have one data set, how do we do this? Well, the simplest way is to use the train/test split from our lecture. Let's give that a try and then we can measure the **Mean Squared Error** of our predictions to our actual prices. Furthermore, let's use the rest of our features besides just number or rooms."]},{"cell_type":"code","metadata":{"id":"h3WaeO8Wpvoe","colab_type":"code","colab":{}},"source":["#Train/test split, build multiple linear regression model and calculate mean squared error\n","X = raw.iloc[:, :-1].values \n","y = raw['MEDV'].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","mlm = LinearRegression()\n","\n","mlm.fit(X_train, y_train)\n","y_train_pred = mlm.predict(X_train) \n","y_test_pred = mlm.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTH82N1rpvq9","colab_type":"text"},"source":["Visualizing all the predictor features against the target is difficult, so we will rely on the **MSE** as our guide to fit. We can see both the **MSE** on the training and testing data as shown:"]},{"cell_type":"code","metadata":{"id":"NzT9eo1qpvt1","colab_type":"code","colab":{}},"source":["#Calculate MSE\n","print('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred), \n","                                       mean_squared_error(y_test, y_test_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jp4hGke9pvwU","colab_type":"text"},"source":["Let just see what the coefficients for our new model are and the **R-squared** statistics are for our training and testing data."]},{"cell_type":"code","metadata":{"id":"6JP7eDH-pvyl","colab_type":"code","colab":{}},"source":["#Print coefficients\n","pd.DataFrame(list(zip(raw.columns, mlm.coef_)), columns=['features','coef'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"atg2kfznpv39","colab_type":"code","colab":{}},"source":["#Calculate R-squared\n","print('R^2 train: %.3f, test: %.3f' % (r2_score(y_train, y_train_pred), \n","                                       r2_score(y_test, y_test_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9He7gAKpv6e","colab_type":"text"},"source":["So remember the coefficient for **RM** in our first model was ~9.1. Now in the presence of additional features to explain median home price, the value drops to ~4.1. This is typical in linear regression because the presence (or absence) of other predictor features modifies the estimates -- we have additional information and aren't viewing things inside a \"vacuum.\"\n","\n","The **R-squared** values have also increased meaning this model with 13 predictors explains more variation in the price than the model with just number of rooms. Don't get too excited -- **R-squared** will *always* increase with more features which makes intuitive sense because we are adding additional information. Even if it isn't that predictive, it is still more than we knew without it. The **Adjusted R-squared** is a statistic that penalizes additional predictors in the model and gives a better idea of fit. Investigate on your own!"]},{"cell_type":"markdown","metadata":{"id":"2v8uK0WWpv9V","colab_type":"text"},"source":["##Logistic Regression - Test/Train\n","Now let's revisit our logistic regression model and again try and test how well our model works on unseen observations. Will it be able to classify them correctly as \"high\" and \"low priced\" homes? Remember we had to recode our target variable **MEDV** to take on values of 0 (\"low priced\") and 1 (\"high priced\") so let's perform that operation again before we continue:"]},{"cell_type":"code","metadata":{"id":"vkgwXdJqpwCV","colab_type":"code","colab":{}},"source":["#Binary coding of target  \n","raw['MEDV'] = np.where(raw['MEDV'] < 21.2, 0, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSPLE-_XICWh","colab_type":"text"},"source":["Let's start simple like we did with our linear regression example and use **RM** to predict the probability of a low or high priced home."]},{"cell_type":"code","metadata":{"id":"BBJbIGsHMcIn","colab_type":"code","colab":{}},"source":["#Build and visualize a simple logistic regression\n","X = raw[['RM']].values \n","y = raw['MEDV'].values\n","\n","lr = LogisticRegression()\n","lr.fit(X, y)\n","\n","def log_regplot(X, y, model):\n","    plt.figure(figsize=(15,10))\n","    sns.regplot(X, y, model, logistic=True, color='navy')\n","    return None\n","\n","log_regplot(X, y, lr)\n","plt.xlabel('Avg. Rooms')\n","plt.ylabel('Probability')\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2n5SSqeGyLW","colab_type":"text"},"source":["Similar to the linear regression example, let's add back in the rest of the predictor features and split our data set into training and testing sets. Since this isn't a regression problem we'll need to use a different measure to determine how well our model classifies low and high priced homes. This is where the **confusion matrix** we spoke about in our lecture becomes useful."]},{"cell_type":"code","metadata":{"id":"Twatb60GILcz","colab_type":"code","colab":{}},"source":["#Train/test split, build multiple logistic regression model and build confusion matrices\n","X = raw.iloc[:, :-1].values \n","y = raw['MEDV'].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","mlr = LogisticRegression()\n","\n","mlr.fit(X_train, y_train)\n","y_train_pred = mlr.predict(X_train) \n","y_test_pred = mlr.predict(X_test)\n","\n","trainmtx = confusion_matrix(y_train, y_train_pred)\n","testmtx = confusion_matrix(y_test, y_test_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"osFds07LJK8M","colab_type":"code","colab":{}},"source":["#Training data confusion matrix\n","print(trainmtx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCjOmlygME3y","colab_type":"code","colab":{}},"source":["#Testing data confusion matrix\n","print(testmtx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZCrBsk4PLmU","colab_type":"text"},"source":["Remember that the values along the diagonal indicate where there is agreement between our predictions and our actual target values of 0 and 1. While it looks like we are doing well for the most part, we could probably improve these. What steps do you think would help to do this?\n","\n","Alternatively, we can also check the **accuracy** of our model using the code below:"]},{"cell_type":"code","metadata":{"id":"Oy3drXA8M7Z6","colab_type":"code","colab":{}},"source":["#Calculate model accuracy\n","print('Accuracy train: %.3f, test: %.3f' % (accuracy_score(y_train, y_train_pred),\n","                                           (accuracy_score(y_test, y_test_pred))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlsUEo3t7lKp","colab_type":"text"},"source":["So our accuracy score which is **True Positives+True Negatives/All Observations** is really not bad in both our training and testing data sets. This is probably our best measure since **R-squared** won't work with logistic regression (although a measure called *pseudo R-squared* exists, we won't cover it). Is there anything we could do to improve accuracy?"]},{"cell_type":"markdown","metadata":{"id":"YFSL6ojVO67l","colab_type":"text"},"source":["##Decision Trees - Test Train\n","Finally, let's try our train/test cross-validation technique on decision trees.\n","\n","Decision trees are also widely used as part of **Ensemble Methods** where we build several trees and average their predictions together to arrive at a quantitative or qualitative outcome that is almost always more accurate than a single tree alone (remember these methods are called **Bagging, Boosting, and Random Forest**).\n","\n","Since we are already becoming comfortable with the Boston Housing Prices data set, let's continue to use that. We already converted our target variable, **MEDV**, to a binary outcome so for the sake of time we will use decision trees simply for classification."]},{"cell_type":"code","metadata":{"id":"jds1oG6GHIRg","colab_type":"code","colab":{}},"source":["#Build and visualize a decision tree classifier\n","X = raw.iloc[:, :-1].values\n","y = raw['MEDV'].values\n","\n","dt = DecisionTreeClassifier()\n","dt.fit(X, y)\n","\n","dt_viz = tree.export_graphviz(dt, \n","                              out_file=None, \n","                              feature_names=raw.columns[0:13],  \n","                              class_names=['low','high'],  \n","                              filled=True, \n","                              rounded=True,\n","                              proportion=True,\n","                              special_characters=True)\n","\n","graph = graphviz.Source(dt_viz)\n","graph.format = 'png'\n","graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mpoNKJOdY2Un","colab_type":"text"},"source":["As we've done before, let's split our data -- one set for training the tree and another to test its performance:"]},{"cell_type":"code","metadata":{"id":"U6A0sS1DaAUG","colab_type":"code","colab":{}},"source":["#Train/test split, build decision tree, and evaluate accuracy\n","X = raw.iloc[:, :-1].values \n","y = raw['MEDV'].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","dt = DecisionTreeClassifier(max_depth=3,\n","                            min_samples_leaf=25,\n","                            random_state=0) #These can be adjusted but don't touch \"random_state\" as this locks in our randomization\n","\n","dt.fit(X_train, y_train)\n","y_train_pred = dt.predict(X_train) \n","y_test_pred = dt.predict(X_test)\n","\n","#Calculate model accuracy\n","print('Accuracy train: %.3f, test: %.3f' % (accuracy_score(y_train, y_train_pred),\n","                                           (accuracy_score(y_test, y_test_pred))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ry7gUKtY89Al","colab_type":"text"},"source":["Play with the stopping/pruning parameters to see how it impacts the train/test accuracy. We can see typically our training accuracy is better than our testing accuracy. Again this probably indicates *overfitting* but hopefully we can *tune* our model to achieve the best possible accuracy. Try a few different approaches. Does anything seem to work best?"]},{"cell_type":"markdown","metadata":{"id":"UJG_KnDQ9a3i","colab_type":"text"},"source":["##Your Turn!!\n","Now that we've gone through how to test our models, give it a shot on your own. Using the **Admission_Predict.csv** data set, prepare a model to predict our target variable (**Chance of Admit**) using one of the models above (or more if you have time). You'll have to load and clean the data like we did before, but you can reuse the same code from previous labs."]},{"cell_type":"code","metadata":{"id":"9NQKYPhC-m_F","colab_type":"code","colab":{}},"source":["#Upload the 'Admission_Predict.csv' file from your local computer \n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxr0p0KZ-sH8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fuKhFNx_FOe","colab_type":"text"},"source":["**What model did you choose and why? Write your code below adjusting model parameters as you see fit. What is your final model's accuracy? Are you happy with it or do you think you can do better?**"]},{"cell_type":"code","metadata":{"id":"U3bm13hj_EVw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8zvNNmB_j0p","colab_type":"text"},"source":["##Ensemble Methods\n","Ensemble methods are like getting a second opinion from the doctor only this time the \"doctor\" is a machine learning model. These techniques allow us to get the most out of our training data, especially if we only have a limited amount. There are several techniques for building ensembles, but we will cover three of the most common below. While **Random Forest** ensembles are limited to desicion tree algorithms, both **Bagging** and **Boosting** can be applied to other models."]},{"cell_type":"markdown","metadata":{"id":"4XLi8uRAap4n","colab_type":"text"},"source":["\n","###Bagging\n","\n","Remember with bagging, we are simulating several data sets by sampling from our one set (with replacement) and building multiple trees with the new data sets then testing them with the observations we didn't include in each subset. Thankfully running a bagging ensemble is really straightforward in scikit-learn."]},{"cell_type":"code","metadata":{"id":"Kay_hzsqb8JJ","colab_type":"code","colab":{}},"source":["#Train a decision tree with bagging and estimate the Out-of-bag Score\n","bag_tree = BaggingClassifier(dt, \n","                             oob_score=True, \n","                             random_state=0)\n","\n","bag_tree.fit(X, y)\n","print(bag_tree.oob_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zflzz43DdYg_","colab_type":"text"},"source":["So using bagging actually *lowers* our accuracy. Why do you think this is happening in this case? With OOB error, each tree we build might have a different number of observations used to test since we are only using those *not* used to build the tree. What if we just go back to using the accuracy score we've used previously?"]},{"cell_type":"code","metadata":{"id":"NCtCSTjceT2f","colab_type":"code","colab":{}},"source":["#Train a decision tree with bagging and estimate the accuracy\n","bag_tree = BaggingClassifier(dt, random_state=0)\n","\n","bag_tree.fit(X, y)\n","y_pred = bag_tree.predict(X) \n","\n","#Calculate model accuracy\n","print('Accuracy: %.3f' % (accuracy_score(y, y_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XR84O1XHgFzf","colab_type":"text"},"source":["This is a *much* better outcome! While our training accuracy only improved a little, or testing accuracy has improved a lot more meaning bagging has reduced our overfitting. Why do you think that is based on what you know about bagging?\n","\n","###Random Forest\n","\n","Random forests are similar to bagging only now we are also taking random subsets of the *features* rather than using them all. This has the effect of *decorrelating* each tree from each other. If one predictor feature is really strong at predicting the outcome it will be used every time in the bagging ensemble meaning that each of trees has some level of *sameness* to it. Let's see how random forest works with our classification problem using default settings (10 trees, no limit on `min_samples_leaf` or `mean_samples_split`, all features available for random selection."]},{"cell_type":"code","metadata":{"id":"VceQ4ZJYl31Z","colab_type":"code","colab":{}},"source":["#Train a decision tree with random forest and estimate the accuracy\n","rf_tree = RandomForestClassifier(random_state=0)\n","\n","rf_tree.fit(X, y)\n","y_pred = rf_tree.predict(X)\n","\n","#Calculate model accuracy\n","print('Accuracy: %.3f' % (accuracy_score(y, y_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g25jOt5Tn7af","colab_type":"text"},"source":["Holy smokes! That is some good classifying and it doesn't appear to be overfitting our testing accuracy either -- it's actually doing *better* and getting all predictions correct. As an analyst, you should be suspicious of these types of perfect outcomes. What do you think  -- should we be fine with this or investigate further?\n","\n","###Boosting\n","\n","Let's try to build a boosted decision tree model now. With bagging we build all of our trees at the same time with the different subsets of data. Random forest was similar only we randomly selected features as well. With boosting we build trees *sequentially* meaning that we build one, see how it performs, make some tweaks to try an improve the misclassification rate on those observations we predicted incorrectly. Let's try it using default settings."]},{"cell_type":"code","metadata":{"id":"uNZu1fKxrxm8","colab_type":"code","colab":{}},"source":["#Train a decision tree with boosting and estimate the accuracy\n","boo_tree = GradientBoostingClassifier(random_state=0)\n","\n","boo_tree.fit(X, y)\n","y_pred = boo_tree.predict(X) \n","\n","#Calculate model accuracy\n","print('Accuracy: %.3f' % (accuracy_score(y, y_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxywIdSo_3F1","colab_type":"text"},"source":["It looks like both random forest and boosted decision tree models are performing well compared to our single decision tree and the bagged version. I encourage you to explore the settings on the scikit-learn website so you can play around with (and maybe even improve) the results.\n"]}]}