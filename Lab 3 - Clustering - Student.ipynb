{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 3 - Clustering - Student.ipynb","version":"0.3.2","provenance":[{"file_id":"1xyPMrCPY_NMsYFZrlUZSVoCHw6KErtxw","timestamp":1536694924631},{"file_id":"1JCH33rfygsL_Bhi8duxnjePoVfKozltf","timestamp":1535650921481}],"collapsed_sections":["yT2ecg5qp2Mf"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"xjxEHlRMpS5G","colab_type":"text"},"cell_type":"markdown","source":["# Lab 3 - Clustering\n"]},{"metadata":{"id":"yT2ecg5qp2Mf","colab_type":"text"},"cell_type":"markdown","source":["##Clustering\n","\n","Now let's move to unsupervised learning. Here we aren't trying to predict an outcome. Instead we just want to understand relationships between our observations. Remember that clustering is a technique used for things like market segmentation. What if we repurpose our housing data set for clustering? Say now we are trying to find similar census tracts (i.e. our observations) so we can mail them some marketing information. \n","\n","Luckily all our data set features are numeric so we shouldn't have issues with the algorithm. \n","\n","##Shortcuts\n","*   Use the \"+ Code\" button in the top left corner to add another block like this one only for running **code** or the \"+ text\" button for adding a block that runs **text**\n","*   I suggest looking at \"Tools-->Keyboard Shortcuts...\" for additional ways to run Colaboratory but here are a few useful ones:\n","> **Ctrl+F9** - Run all blocks   \n","> **Ctrl+Enter** - Run selected block   \n","> **Alt+Enter** - Run block and add a new block beneath   \n","> **Shift+Enter** - Run block and select next block   \n","> **Ctrl+F8** - Run all blocks before selected block   \n","> **Ctrl+F10** - Run selected block and all following blocks   \n","> **Ctrl+M+Y** - Convert selected block to a *code* block   \n","> **Ctrl+M+M** - Convert selected block to a *text* block\n","\n","Also useful, Colaboratory supports code completion. Start typing code and press the **Tab** key. A drop down will appear with likely code based on what you typed. If only one possible command exists, it should complete it for you automatically.\n","\n","Even better yet, if there is an error produced by your code, Colaboratory will provide a button at the bottom of the code output to search StackOverflow for an answer!\n","\n","**Remember**: Code blocks need to be run in succession or they might produce errors!"]},{"metadata":{"id":"8RfFsPwk8o5T","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load our packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import files\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import confusion_matrix, accuracy_score, silhouette_score\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn import datasets\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","%matplotlib inline "],"execution_count":0,"outputs":[]},{"metadata":{"id":"kf6yoJ4fH_FD","colab_type":"code","colab":{}},"cell_type":"code","source":["#Upload the 'HousingData.csv' file from your local computer \n","files.upload()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BDkNr5UiTWl8","colab_type":"code","colab":{}},"cell_type":"code","source":["#Assign data to object\n","raw = pd.read_csv('HousingData.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7dEdVZ2FRt2q","colab_type":"code","colab":{}},"cell_type":"code","source":["#Repeat the data cleaning activites from Lab 1\n","raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']] = raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']].fillna(raw.mean())\n","\n","for col in ['ZN','CHAS']:\n","    raw[col].fillna(raw[col].mode()[0], inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x3zcQP95wLPd","colab_type":"text"},"cell_type":"markdown","source":["##K-means"]},{"metadata":{"id":"ATY7-abSB_yv","colab_type":"text"},"cell_type":"markdown","source":["Before we begin, let's take another look at our data and see if there are any variables that appear to make good clusters. We can do this using the **pairplot** graph we used in Lab 1. Visualizing multidimensional data is difficult so for the sake of demonstration let's choose just two variables from our data set that look like they might cluster well. In real life we wouldn't have this convenience."]},{"metadata":{"id":"DYxvVztqCuuG","colab_type":"code","colab":{}},"cell_type":"code","source":["#Check correlations between features\n","sns.pairplot(raw, size=1.5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zhfoo3jWD2Sw","colab_type":"text"},"cell_type":"markdown","source":["I'm going to chose to use **B** and **NOX** as they appear to have 2-clusters naturally. Let's build a k-means model and specify these details in our code:"]},{"metadata":{"id":"wmT3u-cBv6t6","colab_type":"code","colab":{}},"cell_type":"code","source":["#Train a k-means cluster model with 2 clusters\n","kmeans = KMeans(n_clusters=2)\n","\n","X = raw[['B','NOX']]\n","y_pred = kmeans.fit_predict(X)\n","\n","cmap = 'tab10' #This just sets the color palette\n","plt.figure(figsize=(15,10))\n","plt.scatter(X.iloc[:,0], X.iloc[:,1], c=y_pred, cmap=cmap)\n","plt.xlabel('B')\n","plt.ylabel('NOX')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"68lAsy-uwzC6","colab_type":"text"},"cell_type":"markdown","source":["So it seems that k-means is also finding the same two clusters we can visually see. Clearly, there appears to be some imbalance between the two but the objective of k-means is only to find groupings -- whether or not the contain they same number of data points.\n","\n","Let's try again with 3 clusters. Since it appears that 2 clusters is the *ideal* soultion, let's see what we get when we force k-means to find an extra cluster:"]},{"metadata":{"id":"ivS4J8Bfy-RU","colab_type":"code","colab":{}},"cell_type":"code","source":["#Train a k-means cluster model with 3 clusters\n","kmeans = KMeans(n_clusters=3)\n","\n","y_pred = kmeans.fit_predict(X)\n","\n","cmap = 'tab10'\n","plt.figure(figsize=(15,10))\n","plt.scatter(X.iloc[:,0], X.iloc[:,1], c=y_pred, cmap=cmap)\n","plt.xlabel('B')\n","plt.ylabel('NOX')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xgIhweJS4T9o","colab_type":"text"},"cell_type":"markdown","source":["Now you can see how k-means forces all data points into one of the clusters. Since we have to specify the number of clusters a priori we get arbitrary splits in the data as shown above. It is senseless.\n","\n","Let's simulate some data where we can show a good clustering vs. a bad one using k-means:"]},{"metadata":{"id":"DNuJaVwQ5D-l","colab_type":"code","colab":{}},"cell_type":"code","source":["#Create toy data sets\n","centers_neat = [(-10, 10), (0, -5), (10, 5)]\n","x_neat, _ = datasets.make_blobs(n_samples=5000,\n","                                centers=centers_neat,\n","                                cluster_std=2,\n","                                random_state=2)\n","\n","x_messy, _ = datasets.make_classification(n_samples=5000,\n","                                          n_features=10,\n","                                          n_classes=3,\n","                                          n_clusters_per_class=1,\n","                                          class_sep=1.5,\n","                                          shuffle=False,\n","                                          random_state=301)\n","#Default plot params\n","plt.style.use('seaborn')\n","cmap = 'tab10'\n","\n","plt.figure(figsize=(15,8))\n","plt.subplot(121, title='\"Neat\" Clusters')\n","plt.scatter(x_neat[:,0], x_neat[:,1])\n","plt.subplot(122, title='\"Messy\" Clusters')\n","plt.scatter(x_messy[:,0], x_messy[:,1])\n","\n","km_neat = KMeans(n_clusters=3, random_state=0).fit_predict(x_neat)\n","km_messy = KMeans(n_clusters=3, random_state=0).fit_predict(x_messy)\n","\n","plt.figure(figsize=(15,8))\n","plt.subplot(121, title='\"Neat\" K-Means')\n","plt.scatter(x_neat[:,0], x_neat[:,1], c=km_neat, cmap=cmap)\n","plt.subplot(122, title='\"Messy\" K-Means')\n","plt.scatter(x_messy[:,0], x_messy[:,1], c=km_messy, cmap=cmap)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GmJg8nr_5a_O","colab_type":"text"},"cell_type":"markdown","source":["When our data is roughly spherical and separated, k-means works fine. When it's the opposite, not so much!\n","\n","##Your Turn!\n"]},{"metadata":{"id":"2xKMzznG8h5p","colab_type":"text"},"cell_type":"markdown","source":["**Let's start by creating some artificial data clusters using the scikit-learn.datasets sample generator used above (learn more and discover other data generators [here](http://scikit-learn.org/stable/datasets/index.html#sample-generators).**"]},{"metadata":{"id":"GkzBdv9Wr8RL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"JWsW5Dddr8ls","colab_type":"text"},"cell_type":"markdown","source":["**Using the data you generated, run a k-means clustering and explain the outcome. How many clusters did you choose? Are you satisfied with these clusters? Why?**"]},{"metadata":{"id":"onbCEWHFsLe8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"cEkGfmNosMAi","colab_type":"text"},"cell_type":"markdown","source":["**One method for determining how well a k-means clustering worked is called the \"silhouette score.\" The score is computed by computing the mean distance between each data point and the other data points in its cluster and comparing that to the mean distance between each data point with points in the nearest cluster it *doesn't* belong to. Formally:** \n","\n","`(mean nearest cluster distance - mean intra-cluster distance)/max(nearest, intra-cluster)` \n","\n","**We've already loaded the silhouette score module in the first code cell above. The function works by passing the data set object, the labels assigned by the k-means algorithm, and the distance measure to it to return a value between -1 (bad clustering) to 1 (good clustering). For example:**\n","\n","`metrics.silhouette_score(data set, labels, metric='euclidean')`\n","\n","**Calcluate the score and report it. Are you satisfied or could it be improved?**"]},{"metadata":{"id":"5a_U204avGVZ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"WLppycd6vHEh","colab_type":"text"},"cell_type":"markdown","source":["##Hierarchical Clustering"]},{"metadata":{"id":"NIO0XPMfwVtk","colab_type":"text"},"cell_type":"markdown","source":["Let's try a second method for clustering, hierarchical (or sometimes called agglomerative) clustering. Remember with this type of clustering algorithm, we are *not* selecting the number of clusters beforehand but rather choosing them after the algorithm is run. This removes some of the potential bias of the analyst and can be more helpful in cases where we don't know anything about the potential clusters existing within the data set.\n","\n","We already generated a few sample data sets above for k-means but with 5000 samples our dendrogram might be hard to visualize. Let's use those again only this time using hierarchical clustering with fewer samples. We'll finish be generating a dendrogram to see our results.\n","\n","**NOTE: Scikit-learn *can* perform hierarchical clustering, but plotting a dendrogram isn't a native feature of the function. Therefore, we will be using the package \"Scipy\" for this example.**"]},{"metadata":{"id":"Ak2FsD8LxCd4","colab_type":"code","colab":{}},"cell_type":"code","source":["#Create toy data sets\n","centers_neat = [(-10, 10), (0, -5), (10, 5)]\n","hc_neat, _ = datasets.make_blobs(n_samples=150,\n","                                 centers=centers_neat,\n","                                 cluster_std=1,\n","                                 random_state=2)\n","\n","#Build an Agglomerative Cluster model with default settings\n","clt = linkage(hc_neat, method='single')\n","\n","#Create a function for plotting dendrogram\n","plt.figure(figsize=(31,10))\n","dendrogram(clt, leaf_font_size=10, color_threshold=5)\n","plt.title('Dendrogram of \\'neat\\' clusters')\n","plt.ylabel('Euclidean Distance')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EAkA2p8YxDud","colab_type":"text"},"cell_type":"markdown","source":["Our x-axis is labeled with our data set's row index and our y-axis is the Euclidean distance between data points (this is the default distance measure when using Scipy's 'linkage' function). What can we tell about our data from the dendrogram? How many clusters did we generate with our sample data set? Does our dendrogram look like it can identify that number of clusters? Let's have a look at the data set we generated:"]},{"metadata":{"id":"-wCnyhfA4qwA","colab_type":"code","colab":{}},"cell_type":"code","source":["#Plot our sample data\n","plt.figure(figsize=(12,10))\n","plt.scatter(hc_neat[:,0], hc_neat[:,1])\n","plt.title('Sample data set')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o-XbUxmPEuqU","colab_type":"text"},"cell_type":"markdown","source":["It looks like our dendrogram is identifying three clusters which would agree with what we see when we visulaize our sample data set above. Not bad for only a few lines of code!"]},{"metadata":{"id":"5tgfs8MCGIU0","colab_type":"text"},"cell_type":"markdown","source":["##Your Turn!\n","\n","**Generate a new sample data set and change the parameters to make it a little \"messier\" than what we did above.**"]},{"metadata":{"id":"e0-387dKGbOP","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"DkFI88IlGc41","colab_type":"text"},"cell_type":"markdown","source":["**Create a dendrogram to visualize your clustering and compare it to the data set you generate. What do you see? How many clusters do you think would be appropriate in this situation?**"]},{"metadata":{"id":"xOCnM6SxGr6h","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}