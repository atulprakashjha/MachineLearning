{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 1 - EDA and Regression - Student.ipynb","version":"0.3.2","provenance":[{"file_id":"1VI3bafHFIu4a9gVIwZUr-PfmNR32oZhh","timestamp":1536692463116},{"file_id":"1JCH33rfygsL_Bhi8duxnjePoVfKozltf","timestamp":1535145121111}],"collapsed_sections":["yT2ecg5qp2Mf"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xjxEHlRMpS5G","colab_type":"text"},"source":["# Lab 1 - Exploratory Data Analysis and Regression\n"]},{"cell_type":"markdown","metadata":{"id":"yT2ecg5qp2Mf","colab_type":"text"},"source":["##Introduction\n","\n","Welcome to **Lab 1**! This first of five labs we will cover in the course Machine Learning and Deep Learning Fundamentals with Python. The labs are intended to give you a chance to put to use some of the teachings in the course lecture as well as introduce you to some concepts that are extensions of topics we won't have enough time to cover. Constrained to two days, we cannot cover all the topics in Machine Learning and Deep Learning but if you decide to pursue further education and exploration, the Internet is a vast resource of information.\n","\n","**Lab 1** will cover two topics from our lecture: Exploratory Data Analysis, or EDA, and Regression. We will be using Google's Colaboratory (that's what you're looking at right now) for running code which is an online version of [Jupyter Notebooks](http://jupyter.org/), one of the most widely used reporting tools for data science and analytical projects today. Jupyter allows analysts to mix text with [Python](https://www.python.org/) code and display visualizations all in one spot.\n","\n","##Brief Overview of Collaboratory\n","\n","As you can see, Colaboratory allows us to type text and code on the left and display the result on the right. Within the text, you can see what we call *Markdown* -- things like the pound sign and asterisk that makes text ##headers, **bold text**, and *italicized* text. [Here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) is a good resource for other things you can do with Markdown.\n","\n","Colaboratory allows you to save notebooks like this one on Google Drive for cloud storage as well as colaboration (hence the name, I suspect). Notebooks are run in the background on virtual machines with Jupyter, Python, and several common Python packages pre-installed (you can install other packages if needed). Additionally, when we need to pack on extra firepower for running Deep Learning models, Colaboratory allows us to use Graphics Processing Units, or GPUs, which can substantially speed up processing.\n","\n","Here are a few shortcuts before we get started:\n","\n","*   Use the \"+ Code\" button in the top left corner to add another block like this one only for running **code** or the \"+ text\" button for adding a block that runs **text**\n","*   I suggest looking at \"Tools-->Keyboard Shortcuts...\" for additional ways to run Colaboratory but here are a few useful ones:\n","> **Ctrl+F9** - Run all blocks   \n","> **Ctrl+Enter** - Run selected block   \n","> **Alt+Enter** - Run block and add a new block beneath   \n","> **Shift+Enter** - Run block and select next block   \n","> **Ctrl+F8** - Run all blocks before selected block   \n","> **Ctrl+F10** - Run selected block and all following blocks   \n","> **Ctrl+M+Y** - Convert selected block to a *code* block   \n","> **Ctrl+M+M** - Convert selected block to a *text* block\n","\n","Also useful, Colaboratory supports code completion. Start typing code and press the **Tab** key. A drop down will appear with code based on what you typed. If only one possible command exists, it should complete it for you automatically. You may also set your cursor inside the parenthesis of a function, press **Tab**, and get notes about that function, its arguments, and sometimes example code. Even better yet, if there is an error produced by your code, Colaboratory will provide a button at the bottom of the code output to search StackOverflow for an answer!\n","\n","**NOTE:** In order to have the code run correctly, you'll need to execute each code block in order by clicking on the \"Play Button\" on the left-hand side of each block!\n"]},{"cell_type":"markdown","metadata":{"id":"19bONXrdrpml","colab_type":"text"},"source":["##Exploratory Data Analysis (EDA)\n","\n","Remember from the lecture that most of a data scientist/ML engineer's time is spent cleaning and preparing data for modeling. Data is inheriently messy and if it isn't cleaned properly, our models will not produce valid estimates.\n","\n","In the lecture it was mentioned that one framework for EDA is **Exploration, Preparation, and Analyzation**:\n","\n","* **Exploration** - What does our data set *look* like? How many observations and features? Missing values? What data types? Etc.\n","* **Preparation** - What do we need to do to clean it up and make it *tidy*?\n","* **Analyzation** - What *statistics* and *relationships* describe our data that might be useful to future modeling? Are there outliers? What assumptions can we make about our data and the models that might work best to solve our problem? \n","\n","We will start by using the **Boston Housing Prices** data set which is commonly used in ML research for both EDA and linear regression. You can upload the file from your local computer. Here is an overview of the packages we will be using:\n","\n","* **[scikit-learn](http://scikit-learn.org/stable/)** - One of the most popular Python packages for ML models. It provides tools for modeling classification, regression, and clustering problems as well as tools for model selection and data preprocessing.\n","* **[NumPy](http://www.numpy.org/)** - This package is also one of the most popular and widely applicable in ML. It provides tools for performing quick linear algebra operations seen in many ML model training algorithms. It's based on C which makes it very fast.\n","* **[Pandas](http://pandas.pydata.org/)** - Pandas allows us to work with data in *data frames* which are the ideal structure for ML data sets -- just like everyone's \"favorite,\" Excel! Pandas has tools for manipulating data frames in many useful ways.\n","* **[Matplotlib](https://matplotlib.org/)** - A package for data visualizations.\n","* **[Seaborn](https://seaborn.pydata.org/)** - Another package for data visualizations built on Matplotlib.\n","\n","Let's get started and load the data!\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"38PcL20cxbGV","colab_type":"code","colab":{}},"source":["!apt-get -qq install -y graphviz && pip install -q pydot\n","import pydot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz20WBVQyMhB","colab_type":"code","colab":{}},"source":["#Install packages not native to Colaboratory\n","!pip -q install graphviz\n","!pip -q install pydot\n","import pydot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RfFsPwk8o5T","colab_type":"code","colab":{}},"source":["#Load our packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import graphviz\n","from google.colab import files\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.cluster import KMeans\n","from sklearn import datasets\n","%matplotlib inline "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eg2gXcovGrsm","colab_type":"text"},"source":["When you run the cell below, you will be prompted to open a file. Please select the file **\"HousingData.csv\"**"]},{"cell_type":"code","metadata":{"id":"kf6yoJ4fH_FD","colab_type":"code","colab":{}},"source":["#Upload the file from your local computer and save it in the Jupyter environment\n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKfDEyQyI5es","colab_type":"code","colab":{}},"source":["#What are the dimensions of our data frame (row, column)?\n","raw = pd.read_csv('HousingData.csv')\n","raw.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjbJphaRJuHK","colab_type":"text"},"source":["We can see that our data set has 506 observations and 14 features. Let's take a snapshot look at the first 15 rows of data:"]},{"cell_type":"code","metadata":{"id":"GL-l7F0wKcQs","colab_type":"code","colab":{}},"source":["#Look at first 15 rows\n","raw.head(15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYbuL3DPKgPG","colab_type":"text"},"source":["We can already see that our data has missing values in some of the features (NaN). What exactly *are* each of these features though? Part of EDA is understanding what we are working with. We can use the info [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) to gain more knowledge of the data set. So please note that each observation is an average measure of homes in a US Census Tract in the Boston area.\n","\n","It looks like this data set is typically used for regression modeling. After we do some cleaning we will try and use the 13 features to predict the 14th, **MEDV** -- the median home price in $1,000s (you can tell that this data set is fairly old with these prices!).\n","\n","By running the **raw.shape** and **raw.head(15)** commands, we've already started our **Exploratory** analysis of EDA. Let's dive deeper and check for two specific issues: missing values and data types.\n"]},{"cell_type":"code","metadata":{"id":"voyw3zofPJVN","colab_type":"code","colab":{}},"source":["#Counts of missing values and data type descriptions\n","print(pd.isnull(raw).sum())\n","print(raw.dtypes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ejlKvwwdMUr","colab_type":"text"},"source":["These two simple commands allow us to see both the count of missing values in our data set as well as the data type for each column. Why is this important? Missing values can be impactful because:\n","\n","1. They may indicate something happened during the data collection process that needs to be investigated before future data is collected\n","2. They may inhibit our ability to build certain models where we require data input from all of our observations\n","\n","Regarding bullet 1, missing values can be either \"missing at random\" or \"missing *not* at random.\" The later might indicate a systematic error in our data which would effedctively limit us from moving forward with any model building. We can't model/predict if we can't trust our data. It is the model lifeblood -- the *oil*.\n","\n","There are a few ways of dealing with missing values:\n","\n","1. Delete the observation with missing values in features we intended to use for modeling. Based on the number of missing values across the whole data set, this might significantly reduce the size of observations we have to work with and thus weaken our model.\n","\n","2. Delete the feature with extensive missing values. We aren't going to get much help from a bunch of NULL values in building a model.\n","\n","3. Impute values. We can use several techniques like replacing missing values with the feature's mean, median, or mode. Imputing it based on the values of *other* features in our data set. We can even use linear or logistic regression to impute continuous and discrete missing values, respectfully.\n","\n","Regarding our data types, we can see that all of our data is numeric. **\"float64\"** indicates a continuous value (i.e. can take on decimal places) while **\"int64\"** indicates discrete numeric values. We don't need to do anything further with this. Data types come in to play when we see things like categorical features saved as **\"strings\"** rather than ordinal, numeric data or dates saved as **\"float64\"**. This can impact how ML algorithms treat them or it may make them not work at all.\n","\n","Back to missing values, it looks like each feature with missing values has 20 of them. This looks suspicious. If individual observations contain all the missing values across features we might just remove them. This may also indicate missing values are *not* at random and we may need to investigate how data is collected further. If the missing values are spread around observations we may just have a \"missing at random\" case which is more favorable to moving forward."]},{"cell_type":"code","metadata":{"id":"_gUkVrmOkKOJ","colab_type":"code","colab":{}},"source":["#View only rows with missing values\n","mv = raw[raw.isnull().any(axis=1)]\n","mv"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2scpLYEFmxd2","colab_type":"text"},"source":["It looks like 112 observations contain at least one missing value so we know that there aren't certain cases with all of them. This is favorable. Let's go ahead and replace them then since removing 112 observations would reduce our data set size much more than just 20. Let's be careful though because two of our features are not like the rest: **ZN** and **CHAS**. The **ZN** feature takes on mostly zero values while **CHAS** is binary (0,1). Replacing with the mean would fundamentally alter these."]},{"cell_type":"code","metadata":{"id":"OSnviZw1zq0r","colab_type":"code","colab":{}},"source":["#Replace subset of columns with column mean\n","raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']] = raw[['CRIM','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']].fillna(raw.mean())\n","raw.iloc[mv.index]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2eUeH1m046K","colab_type":"code","colab":{}},"source":["#Replace subset of columns with column mode\n","for col in ['ZN','CHAS']:\n","    raw[col].fillna(raw[col].mode()[0], inplace=True)\n","\n","raw.iloc[mv.index]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RL6i7DJj4X5u","colab_type":"text"},"source":["This takes care of all the missing values in our columns. Because Python can be \"finicky\" at times you can see that replacing missing values with the mode requires different code than when we replace them with the mean. Each command returns a different data type.\n","\n","So now we've taken care of missing values and explored each feature's data type. What about the \"tidiness\" of our data? Do we have one feature per column, one observation per row? Yes. Our data set is \"tidy!\" This completes both the **Exploration** and **Preperation** steps. Let's check statistics and relationships with our **Analyzation** step.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3AwcI1FT9Wxb","colab_type":"code","colab":{}},"source":["#Check summary statistics\n","raw.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2piJvvPb9Z1c","colab_type":"text"},"source":["The \"describe\" command in Pandas allows us to quickly look at summary statistics for our data set. We can verify that all our features contain 506 values and nothing seems particularly out of the ordinary except for the maximum median home value (**MEDV**) is $50,000. This might be nothing -- the data set website mentions the possibility of \"censoring the data\" but we can't be sure so we'll leave it.\n","\n","Describe gives us measures of central tendency (mean) and dispursion/distribution (standard deviation, range, percentiles) which are typically what we need in terms of statistics. How about relationships between features?\n","\n"]},{"cell_type":"code","metadata":{"id":"pExFSUF1_Ujv","colab_type":"code","colab":{}},"source":["#Check correlations between features\n","sns.pairplot(raw, size=1.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DK3Y2g_k_Z95","colab_type":"text"},"source":["The \"pairplot\" (typically called a \"scatterplot matrix\") allows us to see relationships between all our features. Since we are going to use this data for regression, we might be particularly interested in relationships (in this case correlations) between all features and **MEDV**. At first glance, there could be a relationship between **MEDV** and **RM**, the average number of rooms per home. This makes sense -- a bigger home probably is more expensive. There also seems to be a negative relationship with **MEDV** and **LSTAT**, the percent \"lower status\" of the population. This also makes sense -- more affluent residents probably live in nicer homes. Anything else of interest?\n","\n","There are other visualizations than the pairplot and other data qualites of interest like outlier detection, but for the sake of time, let's continue on. Linear regression might allow us to find outliers anyway."]},{"cell_type":"markdown","metadata":{"id":"BxfNE4HHDkMJ","colab_type":"text"},"source":["##Your Turn!\n","Now is you chance to apply EDA on your own data set. Using the **\"Admission_Predict.csv\"** data set, load it into the notebook and perform some quick EDA utilizing what you learned with the code above. [Here](https://www.kaggle.com/mohansacharya/graduate-admissions/home) is the metadata on the set. When in doubt visit the Python package websites linked above and, of course, every programmer's best friend [StackOverflow](https://stackoverflow.com)!"]},{"cell_type":"code","metadata":{"id":"zXDpit_NEA9l","colab_type":"code","colab":{}},"source":["#Upload the file from your local computer and save it in the Jupyter environment\n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7PSXMn87Jsxm","colab_type":"text"},"source":["**Load the data file into a Pandas dataframe named \"ap\"**"]},{"cell_type":"code","metadata":{"id":"PKcQ_HELIw9w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eom2DsMlJDPW","colab_type":"text"},"source":["**What are the dimensions of this data file?**"]},{"cell_type":"code","metadata":{"id":"_28ovgF1JQPk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z2N0YfEzJQw1","colab_type":"text"},"source":["**Describe the data. Are there missing values? If so, how many? Anything that keeps this structured data set from being \"tidy?\"**"]},{"cell_type":"code","metadata":{"id":"5pn1KB11Ji7j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fimGFe9wNISt","colab_type":"text"},"source":["**Take appropriate steps to prepare the data set for analysis and describe what you did.**"]},{"cell_type":"code","metadata":{"id":"CZnta7V4NacU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQ1EhJpUbXq4","colab_type":"text"},"source":["**What features might be good predictors for \"Chance of Admit?\" Are there any that have little to no predictive value?**"]},{"cell_type":"code","metadata":{"id":"rjCKRLsFbqtH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyJMRfMRDd_p","colab_type":"text"},"source":["##Linear Regression\n","\n","Remember that linear regression is used to take a set of features and try to predict a quantitative outcome, in this case the median price of a home in Boston. Let's do a simple example based on what we saw in our EDA: simple linear regression using the number of rooms to predict the median home value."]},{"cell_type":"code","metadata":{"id":"fWjEBYfrpvcs","colab_type":"code","colab":{}},"source":["#Build and visualize a simple linear regression\n","X = raw[['RM']].values \n","y = raw[['MEDV']].values\n","\n","lm = LinearRegression()\n","lm.fit(X, y)\n","\n","def lin_regplot(X, y, model):\n","    plt.figure(figsize=(15,10))\n","    plt.scatter(X, y, c='navy', edgecolor='white', s=55)\n","    plt.plot(X, model.predict(X), color='black', lw=2.5)\n","    return None\n","\n","lin_regplot(X, y, lm)\n","plt.xlabel('Avg. Rooms')\n","plt.ylabel('Price in $1,000')\n","plt.title('Simple Linear Regression: Average Rooms Predicting Median Home Value')\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XC29UMH-pvgF","colab_type":"text"},"source":["As suspected, as the number of rooms increases, so does price on average. However, we also see that there appear to be several outliers so there is potential our model might not be that great. Let's look at the coefficient for **RM** and the **R-squared** statistic which tells us how much variance in the target feature our model explains:"]},{"cell_type":"code","metadata":{"id":"Q9FZir2XpvjF","colab_type":"code","colab":{}},"source":["#Check coefficients and R-squared\n","print(lm.coef_)\n","print('R-squared =', lm.score(X, y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvjVGV95fo3d","colab_type":"text"},"source":["The coefficient estimated tells us that for each extra room, the median home price increases by about $9,000. Remember, R-squared is a measure of the total variance our regression line explains divided by the total variation in the data -- a ratio, 0 to 1 calculated as **1-(RSS/TSS)**. Closer to \"1\" is better so this simple model is just OK.\n","\n","We can probably do better than this. What happens if we include the rest of our features?"]},{"cell_type":"code","metadata":{"id":"DnQHIsGghz5f","colab_type":"code","colab":{}},"source":["#Multiple linear regression\n","X = raw.iloc[:, :-1].values \n","y = raw['MEDV'].values\n","\n","mlm = LinearRegression()\n","mlm.fit(X, y)\n","\n","print('R-squared =', mlm.score(X, y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VAGfgaRld50","colab_type":"text"},"source":["That's quite an improvement! \n","\n","The **R-squared** values have increased meaning this model with 13 predictors explains more variation in the price than the model with just the number of rooms. Don't get too excited -- **R-squared** will *always* increase with more features which makes intuitive sense because we are adding additional information. Even if it isn't that predictive, it is still more than we knew without it. The **Adjusted R-squared** is a statistic that penalizes additional predictors in the model and gives a better idea of fit. Investigate on your own!\n","\n","What features seem to have the biggest influence on **MEDV?**"]},{"cell_type":"code","metadata":{"id":"Ssb2YfMbldKj","colab_type":"code","colab":{}},"source":["pd.DataFrame(list(zip(raw.columns[:-1], mlm.coef_)), \n","             columns=('Feature','Coefficient'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7qvz_03nBir","colab_type":"text"},"source":["Most notably, the presence of Nitric Oxide (**NOX**) seems to significantly impact the **MEDV** negatively. Other features with a negative impact on **MEDV** make intuitive sense as well like Per Capita Crime Rate (**CRIM**) -- higher rates, lower home values."]},{"cell_type":"markdown","metadata":{"id":"q3PK2es2oBd-","colab_type":"text"},"source":["##Your Turn!\n","Now is you chance to apply regression analysis on your own data set. The **\"Admission_Predict.csv\"** data set should already be loaded into the Python object **\"ap\"** so no need to reload it. \n","\n","**First, choose one feature to predict \"Chance of Admit\". What did you choose and why? Create a linear regression model, plot it, and report the R-squared**"]},{"cell_type":"code","metadata":{"id":"OqMEPP64ocja","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxrtvZWls_wS","colab_type":"text"},"source":["**Now try a multiple linear regression model with all features as predictors. How big is the improvement in fit? Are any features having a greater impact than others?**"]},{"cell_type":"code","metadata":{"id":"VAXjiWHrtO57","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2v8uK0WWpv9V","colab_type":"text"},"source":["##Logistic Regression\n","\n","Since we already cleaned up the **\"HousingData.csv\"** data set, let's try and use it for **Logistic Regression** as well. Remember, with this we are trying to estimate the *probability* that an observation belongs to a certain class -- typically something binary that can be represented as \"Yes/No\" or \"Male/Female\" or \"0/1.\" We *do* have the **CHAS** feature that is binary but it is highly skewed to \"0\" which will make a simple demo hard so we will have to make one! \n","\n","What if we continue to use the **MEDV** feature as our target but now we create an arbitrary dividing line and call all homes below it \"low priced\" and all above \"high priced?\" Let's look again at some summary statisistics to determine where to make the split."]},{"cell_type":"code","metadata":{"id":"vkgwXdJqpwCV","colab_type":"code","colab":{}},"source":["#Check summary statistics\n","raw.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tR5TQMnvpwFN","colab_type":"text"},"source":["If we draw an arbitrary line at the 50th percentile, all observations with homes less than $21,200 would be in the \"low priced\" category and all at or above would be in the \"high priced\" category. Let's try that out."]},{"cell_type":"code","metadata":{"id":"jtD7PqDn15yn","colab_type":"code","colab":{}},"source":["#Replace MEDV with 0/1\n","raw['MEDV'] = np.where(raw['MEDV'] < 21.2, 0, 1)\n","raw['MEDV'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSPLE-_XICWh","colab_type":"text"},"source":["This has effectively given us an even split. In real life, however, this won't always be the case and sometimes we will need to predict outcomes where the split is highly skewed in one direction.\n","\n","Remember in logistic regression we are given the **log-odds** for our coefficients because we want to constrain our outcomes to be between 0 and 1 so that we can interpret them as probabilities. In linear regression we make the assumption that our regression line and therefore the data points it models stretch on to infinity in both directions. This makes no sense in the context of probabilities.\n","\n","Let's start simple like we did with our linear regression example and use **RM** to predict the probability of a low or high priced home."]},{"cell_type":"code","metadata":{"id":"BBJbIGsHMcIn","colab_type":"code","colab":{}},"source":["#Build and visualize a simple logistic regression\n","X = raw[['RM']].values \n","y = raw['MEDV'].values\n","\n","#Don't worry about the 'C' parameter. \n","#It's for what's called 'regularization' which is out of scope for this class\n","lr = LogisticRegression(C=1e8) \n","lr.fit(X, np.ravel(y.astype(int)))\n","\n","plt.figure(figsize=(15,10))\n","plt.scatter(X, y, c='navy', edgecolors='white' )\n","plt.plot(sorted(X), sorted(lr.predict_proba(X)[:,1]), lw=2.5, c='black')\n","plt.xlabel('Number of Rooms')\n","plt.ylabel('Probability')\n","plt.title('Logistic Regression: Probability of High Priced Home Given Rooms')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4kBWeCJGPRKJ","colab_type":"text"},"source":["This is a great plot and easy to understand and draw predictions from, even visually. For instance, we can see that at about 6.25 rooms (remember these are the average number of rooms within the tract) we are on the fence between our home being considered low/high priced. We can also see there are a few homes with 5 or fewer rooms that fall into our \"high priced\" category.\n","\n","We can do a few other interesting things with our model's output. Our intercept and coefficient are in **log-odds** and we'd like to convert that to just **odds** and finally **probabilities**. We can do this in both a long form and short form since scikit-learn has a built in function for calculating the probability of a given value based on our model. Let's calculate the probability of having a \"high priced\" home that has **6** rooms (we can even look at the graph for a visual estimate)."]},{"cell_type":"code","metadata":{"id":"cEyyzQDxa9SX","colab_type":"code","colab":{}},"source":["#Estimate probability of \"high priced\" (long form)\n","rooms = np.exp(lr.intercept_ + (6 * lr.coef_))\n","prob = rooms/(1+rooms)\n","prob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcGqwB6QhG0i","colab_type":"code","colab":{}},"source":["#Estimate probability of \"high priced\" (short form)\n","lr.predict_proba(6)[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZH-I9-nBbfyN","colab_type":"text"},"source":["The function `predict_proba` calculates the probability of \"high priced\" given an input value, in this case \"6 rooms.\" What if we'd like to make similar statements like we did with linear regression, mainly: \"With a 1 unit increase in X, we can expect an increase in Y of...\" With linear regression this is straightforward since we are estimating a straight line with constant slope. Logistic regression, with it's s-shaped line (called a *sigmoid*), our slope *isn't* constant. \n","\n","One method for this interpretation is called the \"Divide by 4 Rule.\" Simply put, the rule states that the maximum slope of our sigmoid is approximately at the half-way point (i.e. 50%). We make the *assumption* of zero slope at this point, work through the math, and arrive at **Beta<sub>1</sub>/4** (since our slope most likely *isn't* zero, this is why it's considered a \"rough estimate\"). Again, since this is considered the *maximum* slope of our sigmoid curve, we consider this probability as an upper limit. The value we calculate is called the **marginal effect**.\n","\n"]},{"cell_type":"code","metadata":{"id":"rtJLIEhPFkHV","colab_type":"code","colab":{}},"source":["#Estimate marginal effect of one room increase on probability of high priced home\n","add_room = (lr.coef_/4)\n","add_room"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyJkt2Bim-OC","colab_type":"text"},"source":["This states: \"With an additional room in our home, the probability of the home being \"high priced\" increases by (at most) **61%**\"\n","\n","We can also see the overall accuracy of our model. \"Accuracy\" in the machine learning sense means how well can our model predict \"True Positives\" (i.e. High Priced homes) *and* \"True Negatives\" (i.e. Low Priced homes) out of all the observations it was modeled on. We'll talk more about model evaluation in a later lab, but for now here is the code:"]},{"cell_type":"code","metadata":{"id":"FqfrgmirGYeV","colab_type":"code","colab":{}},"source":["#Calculate model accuracy\n","lr.score(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3K5sBTnYHMp4","colab_type":"text"},"source":["This states that our model has **74%** accuracy. Not too bad!"]},{"cell_type":"markdown","metadata":{"id":"y2n5SSqeGyLW","colab_type":"text"},"source":["##Your Turn!\n","Now is you chance to apply logistic regression analysis on your own data set. The **\"Admission_Predict.csv\"** data set should already be loaded into the Python object **\"ap\"** so no need to reload it. Remember that logistic regression expects a binary target feature so let's perform a similar data transformation to **\"Chance to Admit\"** like we did with **\"MEDV\"**.\n","\n","**Either create a new feature or recode the existing \"Chance of Admit\" into a binary feature. You can decide where the cutoff threshold should be although the typical default probability is .50 (I chose .73 myself just to balance the target values). What are the frequencies of each group?**"]},{"cell_type":"code","metadata":{"id":"2xKMzznG8h5p","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3i48DsGNu90","colab_type":"text"},"source":["**Fit and plot a logistic regression model using a one predictor feature. What did you choose and why?**"]},{"cell_type":"code","metadata":{"id":"g9vHO5y3O8ZF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PnbAW_Gf0UU","colab_type":"text"},"source":["**What is your probability of admission given your TOEFL Score is 108? What about 104 or 112?**"]},{"cell_type":"code","metadata":{"id":"GlCLcNeCj25B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLyENAebnc5p","colab_type":"text"},"source":["**With a one point increase in TOEFL Score, what is the probability of admission increased by (at most)?**"]},{"cell_type":"code","metadata":{"id":"hAYUTohFnwcr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBcgNcSa20Em","colab_type":"text"},"source":["**What is the overall accuracy of our model?**"]},{"cell_type":"code","metadata":{"id":"jxLkw5QqF6qD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}